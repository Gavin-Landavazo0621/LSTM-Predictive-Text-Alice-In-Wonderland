{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedc9d9d-634d-45cb-ac93-8672669b1d24",
   "metadata": {},
   "source": [
    "# * Work in Progress *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0400c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"C:/Users/gavin/Jupyter/IPYNB/Alice/AliceLand.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0198e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffab7018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  148574\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d6a3237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  148474\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7c9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445768e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb3bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f965a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X.shape[1], X.shape[2]))\n",
    "x = LSTM(128, return_sequences=True)(input_layer)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(128)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output_layer = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107d04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "040c2b8d-ef0d-40a5-abf2-c79e6302461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcac9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - loss: 3.2220\n",
      "Epoch 1: loss improved from inf to 3.09250, saving model to weights-improvement-01-3.0925-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 443ms/step - loss: 3.2210\n",
      "Epoch 2/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 3.0247\n",
      "Epoch 2: loss improved from 3.09250 to 3.01874, saving model to weights-improvement-02-3.0187-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 454ms/step - loss: 3.0246\n",
      "Epoch 3/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 450ms/step - loss: 3.0079\n",
      "Epoch 3: loss improved from 3.01874 to 2.99679, saving model to weights-improvement-03-2.9968-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 450ms/step - loss: 3.0078\n",
      "Epoch 4/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 2.9364\n",
      "Epoch 4: loss improved from 2.99679 to 2.90305, saving model to weights-improvement-04-2.9030-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 456ms/step - loss: 2.9362\n",
      "Epoch 5/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - loss: 2.8077\n",
      "Epoch 5: loss improved from 2.90305 to 2.78377, saving model to weights-improvement-05-2.7838-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 458ms/step - loss: 2.8075\n",
      "Epoch 6/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 2.7173\n",
      "Epoch 6: loss improved from 2.78377 to 2.70293, saving model to weights-improvement-06-2.7029-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 454ms/step - loss: 2.7172\n",
      "Epoch 7/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - loss: 2.6613\n",
      "Epoch 7: loss improved from 2.70293 to 2.65251, saving model to weights-improvement-07-2.6525-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 434ms/step - loss: 2.6612\n",
      "Epoch 8/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - loss: 2.6191\n",
      "Epoch 8: loss improved from 2.65251 to 2.61216, saving model to weights-improvement-08-2.6122-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 438ms/step - loss: 2.6191\n",
      "Epoch 9/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - loss: 2.5855\n",
      "Epoch 9: loss improved from 2.61216 to 2.57879, saving model to weights-improvement-09-2.5788-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 433ms/step - loss: 2.5854\n",
      "Epoch 10/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - loss: 2.5503\n",
      "Epoch 10: loss improved from 2.57879 to 2.54863, saving model to weights-improvement-10-2.5486-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 449ms/step - loss: 2.5503\n",
      "Epoch 11/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 2.5312\n",
      "Epoch 11: loss improved from 2.54863 to 2.52135, saving model to weights-improvement-11-2.5214-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 446ms/step - loss: 2.5312\n",
      "Epoch 12/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - loss: 2.5008\n",
      "Epoch 12: loss improved from 2.52135 to 2.49619, saving model to weights-improvement-12-2.4962-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 452ms/step - loss: 2.5008\n",
      "Epoch 13/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step - loss: 2.4775\n",
      "Epoch 13: loss improved from 2.49619 to 2.47192, saving model to weights-improvement-13-2.4719-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 444ms/step - loss: 2.4774\n",
      "Epoch 14/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - loss: 2.4517\n",
      "Epoch 14: loss improved from 2.47192 to 2.44885, saving model to weights-improvement-14-2.4489-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 441ms/step - loss: 2.4517\n",
      "Epoch 15/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step - loss: 2.4333\n",
      "Epoch 15: loss improved from 2.44885 to 2.42757, saving model to weights-improvement-15-2.4276-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 433ms/step - loss: 2.4332\n",
      "Epoch 16/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - loss: 2.4032\n",
      "Epoch 16: loss improved from 2.42757 to 2.40527, saving model to weights-improvement-16-2.4053-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 429ms/step - loss: 2.4032\n",
      "Epoch 17/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - loss: 2.3918\n",
      "Epoch 17: loss improved from 2.40527 to 2.38426, saving model to weights-improvement-17-2.3843-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 431ms/step - loss: 2.3917\n",
      "Epoch 18/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - loss: 2.3744\n",
      "Epoch 18: loss improved from 2.38426 to 2.36526, saving model to weights-improvement-18-2.3653-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 424ms/step - loss: 2.3744\n",
      "Epoch 19/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - loss: 2.3552\n",
      "Epoch 19: loss improved from 2.36526 to 2.34506, saving model to weights-improvement-19-2.3451-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 429ms/step - loss: 2.3551\n",
      "Epoch 20/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - loss: 2.3332\n",
      "Epoch 20: loss improved from 2.34506 to 2.32730, saving model to weights-improvement-20-2.3273-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 421ms/step - loss: 2.3332\n",
      "Epoch 21/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - loss: 2.3178\n",
      "Epoch 21: loss improved from 2.32730 to 2.31182, saving model to weights-improvement-21-2.3118-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 422ms/step - loss: 2.3177\n",
      "Epoch 22/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - loss: 2.2987\n",
      "Epoch 22: loss improved from 2.31182 to 2.29316, saving model to weights-improvement-22-2.2932-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 427ms/step - loss: 2.2986\n",
      "Epoch 23/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449ms/step - loss: 2.2801\n",
      "Epoch 23: loss improved from 2.29316 to 2.27957, saving model to weights-improvement-23-2.2796-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 449ms/step - loss: 2.2801\n",
      "Epoch 24/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step - loss: 2.2715\n",
      "Epoch 24: loss improved from 2.27957 to 2.26574, saving model to weights-improvement-24-2.2657-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 460ms/step - loss: 2.2715\n",
      "Epoch 25/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 2.2502\n",
      "Epoch 25: loss improved from 2.26574 to 2.24725, saving model to weights-improvement-25-2.2472-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 455ms/step - loss: 2.2502\n",
      "Epoch 26/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451ms/step - loss: 2.2400\n",
      "Epoch 26: loss improved from 2.24725 to 2.23173, saving model to weights-improvement-26-2.2317-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 451ms/step - loss: 2.2400\n",
      "Epoch 27/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - loss: 2.2215\n",
      "Epoch 27: loss improved from 2.23173 to 2.21838, saving model to weights-improvement-27-2.2184-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 430ms/step - loss: 2.2215\n",
      "Epoch 28/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - loss: 2.2040\n",
      "Epoch 28: loss improved from 2.21838 to 2.20539, saving model to weights-improvement-28-2.2054-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 420ms/step - loss: 2.2040\n",
      "Epoch 29/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - loss: 2.1917\n",
      "Epoch 29: loss improved from 2.20539 to 2.19161, saving model to weights-improvement-29-2.1916-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 412ms/step - loss: 2.1917\n",
      "Epoch 30/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - loss: 2.1861\n",
      "Epoch 30: loss improved from 2.19161 to 2.18303, saving model to weights-improvement-30-2.1830-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 402ms/step - loss: 2.1861\n",
      "Epoch 31/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - loss: 2.1688\n",
      "Epoch 31: loss improved from 2.18303 to 2.17073, saving model to weights-improvement-31-2.1707-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 404ms/step - loss: 2.1688\n",
      "Epoch 32/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - loss: 2.1556\n",
      "Epoch 32: loss improved from 2.17073 to 2.15764, saving model to weights-improvement-32-2.1576-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 410ms/step - loss: 2.1556\n",
      "Epoch 33/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - loss: 2.1492\n",
      "Epoch 33: loss improved from 2.15764 to 2.14549, saving model to weights-improvement-33-2.1455-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 413ms/step - loss: 2.1492\n",
      "Epoch 34/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - loss: 2.1296\n",
      "Epoch 34: loss improved from 2.14549 to 2.13662, saving model to weights-improvement-34-2.1366-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 415ms/step - loss: 2.1297\n",
      "Epoch 35/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - loss: 2.1263\n",
      "Epoch 35: loss improved from 2.13662 to 2.12730, saving model to weights-improvement-35-2.1273-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 418ms/step - loss: 2.1263\n",
      "Epoch 36/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step - loss: 2.1174\n",
      "Epoch 36: loss improved from 2.12730 to 2.11800, saving model to weights-improvement-36-2.1180-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 413ms/step - loss: 2.1174\n",
      "Epoch 37/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - loss: 2.1036\n",
      "Epoch 37: loss improved from 2.11800 to 2.10715, saving model to weights-improvement-37-2.1071-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 415ms/step - loss: 2.1036\n",
      "Epoch 38/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 2.0935\n",
      "Epoch 38: loss improved from 2.10715 to 2.09585, saving model to weights-improvement-38-2.0959-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 406ms/step - loss: 2.0935\n",
      "Epoch 39/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 2.0856\n",
      "Epoch 39: loss improved from 2.09585 to 2.08759, saving model to weights-improvement-39-2.0876-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 406ms/step - loss: 2.0857\n",
      "Epoch 40/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - loss: 2.0846\n",
      "Epoch 40: loss improved from 2.08759 to 2.08026, saving model to weights-improvement-40-2.0803-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 405ms/step - loss: 2.0845\n",
      "Epoch 41/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 2.0781\n",
      "Epoch 41: loss improved from 2.08026 to 2.07045, saving model to weights-improvement-41-2.0704-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 406ms/step - loss: 2.0780\n",
      "Epoch 42/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - loss: 2.0714\n",
      "Epoch 42: loss improved from 2.07045 to 2.06424, saving model to weights-improvement-42-2.0642-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 405ms/step - loss: 2.0714\n",
      "Epoch 43/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - loss: 2.0532\n",
      "Epoch 43: loss improved from 2.06424 to 2.05711, saving model to weights-improvement-43-2.0571-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 402ms/step - loss: 2.0533\n",
      "Epoch 44/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - loss: 2.0558\n",
      "Epoch 44: loss improved from 2.05711 to 2.05004, saving model to weights-improvement-44-2.0500-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 403ms/step - loss: 2.0558\n",
      "Epoch 45/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - loss: 2.0368\n",
      "Epoch 45: loss improved from 2.05004 to 2.04252, saving model to weights-improvement-45-2.0425-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 403ms/step - loss: 2.0369\n",
      "Epoch 46/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - loss: 2.0324\n",
      "Epoch 46: loss improved from 2.04252 to 2.03257, saving model to weights-improvement-46-2.0326-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 402ms/step - loss: 2.0324\n",
      "Epoch 47/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - loss: 2.0337\n",
      "Epoch 47: loss improved from 2.03257 to 2.02987, saving model to weights-improvement-47-2.0299-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 407ms/step - loss: 2.0337\n",
      "Epoch 48/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 2.0234\n",
      "Epoch 48: loss improved from 2.02987 to 2.02021, saving model to weights-improvement-48-2.0202-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 406ms/step - loss: 2.0234\n",
      "Epoch 49/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - loss: 2.0128\n",
      "Epoch 49: loss improved from 2.02021 to 2.01177, saving model to weights-improvement-49-2.0118-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 406ms/step - loss: 2.0128\n",
      "Epoch 50/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - loss: 2.0101\n",
      "Epoch 50: loss improved from 2.01177 to 2.00679, saving model to weights-improvement-50-2.0068-bigger.keras\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 409ms/step - loss: 2.0101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ad6bc2f200>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=1200, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ab4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-2.0068-bigger.keras\" ## correct file call\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c2c8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ad been\n",
      "looking at alice for some time with great curiosity, and this was\n",
      "his first speech.\n",
      "\n",
      "  `you  \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05956184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then the sabbit then the sabbit the sabbit the sabbit the sabbit the sooe to the sabbit to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was to the was\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    " x = np.reshape(pattern, (1, len(pattern), 1))\n",
    " x = x / float(n_vocab)\n",
    " prediction = model.predict(x, verbose=0)\n",
    " index = np.argmax(prediction)\n",
    " result = int_to_char[index]\n",
    " seq_in = [int_to_char[value] for value in pattern]\n",
    " sys.stdout.write(result)\n",
    " pattern.append(index)\n",
    " pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4eac7-734d-477d-80cc-e89f59e6428f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
